{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplacians and Clustering\n",
    "\n",
    "<img src=\"tutorials/images/clus_workflow.png\" width=\"850\">\n",
    "\n",
    "Tutorial for hypergraph clustering utilizing random-walk based Laplacians. The hypergraph may be weighted or unweighted. \n",
    "The optional weights are associated with each **vertex-hyperedge pair**, sometimes referred to as \"edge-dependent vertex weights\" or \"cell weights\" of the incidence matrix. If unweighted, the underlying random walk is equivalent to a weighted random walk on the clique expansion (i.e. 2-section, one-mode projection) of the hypergraph. If weights are specified, the random walk isn't necessarily reversible, which implies it cannot be characterized as any random walk on an undirected graph. For more background on Laplacian-based hypergraph clustering, see\n",
    "\n",
    "Hayashi, K., Aksoy, S. G., Park, C. H., & Park, H. \n",
    "Hypergraph random walks, laplacians, and clustering. \n",
    "In Proceedings of CIKM 2020, (2020): 495-504.\n",
    "\n",
    "and the references contained therein. Feel free to direct inquries concerning this tutorial to Sinan Aksoy, sinan.aksoy@pnnl.gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hypernetx as hnx\n",
    "import networkx as nx\n",
    "from hypernetx import Entity\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unweighted hypergraph clustering on LesMis\n",
    "\n",
    "A toy example of unweighted hypergraph clustering characters in the LesMis example based on scene co-occurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = {\n",
    "    0: ('FN', 'TH'),\n",
    "    1: ('TH', 'JV'),\n",
    "    2: ('BM', 'FN', 'JA'),\n",
    "    3: ('JV', 'JU', 'CH', 'BM'),\n",
    "    4: ('JU', 'CH', 'BR', 'CN', 'CC', 'JV', 'BM'),\n",
    "    5: ('TH', 'GP'),\n",
    "    6: ('GP', 'MP'),\n",
    "    7: ('MA', 'GP')\n",
    "}\n",
    "\n",
    "H = hnx.Hypergraph(scenes)\n",
    "hnx.spec_clus(H,3) #3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnx.draw(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted hypergraph clustering\n",
    "\n",
    "Hypergraph clustering on term-document data using tf-idf as cell weights. In this example, we use the 20newsgroups dataset:\n",
    "\n",
    "https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
    "\n",
    "and consider documents falling into two subcategories. We form a hypergraph with 787 documents as vertices, 20,868 terms as hyperedges, and tf-idf as vertex-hyperedge (i.e. cell) weights. We then form the normalized Laplacian and apply the spectral clustering algorithm as defined by \"RDC-Spec\" (Algorithm 1) in:\n",
    "\n",
    "Hayashi, K., Aksoy, S. G., Park, C. H., & Park, H. \n",
    "Hypergraph random walks, laplacians, and clustering. \n",
    "In Proceedings of CIKM 2020, (2020): 495-504.\n",
    "\n",
    "We plot the proportions of the document subcategories within each output cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list possible categories to choose from\n",
    "pprint(list(fetch_20newsgroups(subset='test').target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select categories of documents to be clustered\n",
    "categories=['soc.religion.christian','comp.graphics']\n",
    "twenty_train = fetch_20newsgroups(subset='test',\n",
    "                                  categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record categories of documents\n",
    "doc_types=dict()\n",
    "for i,x in enumerate(twenty_train.filenames):\n",
    "    doc_types[i]=x.split('/')[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#form TF-IDF term-document matrix\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vect.fit_transform(twenty_train.data)\n",
    "X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract vertex-hyperedge incidences and weights from TFIDF matrix\n",
    "mat = coo_matrix(X_tfidf)\n",
    "edges = mat.col\n",
    "nodes = mat.row\n",
    "data = np.array([edges,nodes]).T\n",
    "weights = mat.data\n",
    "\n",
    "h = hnx.Hypergraph(hnx.StaticEntitySet(data=data,weights=weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the hypergraph is connected, as this is required by spectral clustering\n",
    "#if not, restrict to largest connected component or modify hypergraph as necessary\n",
    "h.is_connected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # outputs the cell weight of a selected node in a selected edge\n",
    "# weight = lambda self, node, edge: self.elements[edge].cellweights[node]\n",
    "\n",
    "#the weighted incidence matrix which contain the cell weights\n",
    "I,verMap,edgeMap = h.incidence_matrix(weights=True,index=True)\n",
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster the vertices (documents)\n",
    "num_clus=len(categories)\n",
    "clusters=hnx.spec_clus(h,num_clus,weights=True)\n",
    "print([len(v) for v in clusters.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize document category composition within each cluster\n",
    "fig = plt.figure(figsize=(22, 10))\n",
    "labels=sorted(list(set(doc_types.values())))\n",
    "colors=['#e41a1c','#377eb8']\n",
    "\n",
    "#pie charts\n",
    "for clus in range(num_clus):\n",
    "    ax=plt.subplot(131+clus)\n",
    "    counts=[[doc_types[y] for y in clusters[clus]].count(z) for z in labels]\n",
    "    ax.pie(counts, colors=colors, shadow=True, startangle=90)\n",
    "    ax.set_title(\"Cluster \"+repr(clus)+ \"\\n\"+ repr(sum(counts))+ ' Documents',fontsize=20)\n",
    "\n",
    "#legend\n",
    "ax=plt.subplot(131+num_clus)\n",
    "patches = [mpatches.Patch(color=color, label=label)\n",
    "    for label, color in zip(labels, colors)]\n",
    "ax.legend(patches, labels, loc='center',fontsize=20, frameon=False)\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b11832e3fb1d317fabfdf226ff96dd8761e1caa77b8bb75a64cd45c858a9356"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
